#!/bin/bash
#SBATCH -p gpu # -p for 'partition name'
     # gpu 24 hrs default, 7 d max, 2 GPUs max, 192GB per GPU
#SBATCH --qos=gpu # QOS to be allowed to use GPU servers
#SBATCH --gres=gpu:2 # Number of GPUs (per node)  # gpu:1 is the default. if I set gpu:1, but i use n_gpu=2 in sjsdm_cv() code, sjsdm_cv() will grab 2 gpus, including from jobs by other users
#SBATCH --mem 96G #
#SBATCH --ntasks=2 # number of slots == number of CPU cores (match n_cpu)
#SBATCH -t 48:00:00 # 48 hrs # time (DD-HH:MM)
#SBATCH --job-name=sjsdmcvl2 # job name
#SBATCH -o sjsdmcvl2.out
#SBATCH -e sjsdmcvl2.err
#SBATCH --mail-user=dougwyu@mac.com #sends email to me
#SBATCH --mail-type=ALL           #Mail events (NONE, BEGIN, END, FAIL, ALL)

# ran on 20200830 on ada gpu, 17.5 hrs to complete 274 spp, 87 sites, gismslidar, loocv for each dataset (qp, pa)
# sbatch sjsdm_gpu_20200906.sub # to submit the job

module purge
# source /gpfs/home/b042/scratch/sjSDM_env/bin/activate # activate sjSDM environment to make pytorch available
module add R

# upload sjsdm_gpu_20200906.sub and sjsdm_gpu_20200906.R into working folder (~/sjSDM)
Rscript --vanilla sjsdm_cv_gpu_20200906.R
# this script reruns the lidar cross validation but this time with the XY.csv dataset NOT scaled and centered. testing to see if the crossvalidation results are the same as with the scaled XY.csv data. if they are not the same, then i need to rerun loocv for everything. 
